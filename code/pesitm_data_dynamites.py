# -*- coding: utf-8 -*-
"""PESITM-DATA_DYNAMITES.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZcOIjPXBENWeqJ3k4iljjwUf4AThUYX1

# Quantum Detective: Cracking Financial Anomalies
Detect fraud in the credit card dataset using a Quantum Random Forest Model.

## Pre-processing the data

### Library Imports
I am using the sci-kit learn for classical Random-Forest Model and utilities like train-test split, scaler & metrices
"""

pip install pennylane --upgrade

import pennylane as qml
from pennylane import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.feature_selection import SelectKBest, f_classif
import matplotlib.pyplot as plt
import seaborn as sns

"""### Data Imports & Visualization
The Credit Card Fraud Detection Data is a highly imbalanced data we need to DownSample the data for uniformity to avoid bais learning
"""

data = pd.read_csv('/content/creditcard.csv')
plt.figure(figsize=(10, 5))
sns.countplot(x='Class', data=data)
plt.title('Class Distribution: Legitimate vs Fraud')
plt.show()
plt.figure(figsize=(12, 10))
corr_matrix = data.corr()
sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')
plt.title('Feature Correlation Matrix')
plt.show()

legit = data[data.Class == 0]
fraud = data[data.Class == 1]
legit_sample = legit.sample(n=1000, random_state=42)
data = pd.concat([legit_sample, fraud], axis=0)
plt.figure(figsize=(10, 5))
sns.countplot(x='Class', data=data)
plt.title('Class Distribution: Legitimate vs Fraud after sampling')
plt.show()
X = data.drop('Class', axis=1)
y = data['Class']

selector = SelectKBest(f_classif, k=10)
X_selected = selector.fit_transform(X, y)
selected_features = X.columns[selector.get_support()].tolist()

"""### Train-Test split
We divided the dataset into 80% for training and 20% for testing, which is a common practice in machine learning. This split ensures that there is a sufficient amount of data for the model to learn from, while also keeping enough data aside to effectively evaluate its performance.
"""

X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, stratify=y, random_state=42)

"""### Scaling the Data
The StandardScaler adjusts the feature values so that they are centered around a mean of 0 and have a standard deviation of 1. This ensures the data is on a consistent scale, making it easier for machine learning models to process effectively.
"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Selected Features:", selected_features)

"""## Building the Quantum Ciruit

**Note:** I've chosen to use 10 qubits here since we are running the model on a simulator rather than an actual quantum computer, which helps manage the training time efficiently. Additionally, we selected 10 features from the total of 30, allowing us to represent each feature as a qubit.

**Encoding:** To encode the features into quantum states, I'm using the rotational-y gate.

**Layers:** I limited the model to 2 layers because the dataset, after sampling, is quite small, making a deep model unnecessary. I've also included an entanglement layer to ensure that all 10 qubits are interconnected, so the result isn't based solely on one qubit, even though we are looking for a binary output.
"""

num_qubits = 10
dev = qml.device('default.qubit', wires=num_qubits)

@qml.qnode(dev)
def quantum_circuit(inputs, weights):
    for i in range(num_qubits):
        qml.RY(inputs[i], wires=i)

    for layer in range(2):
        for i in range(num_qubits):
            qml.RX(weights[layer][i][0], wires=i)
            qml.RZ(weights[layer][i][1], wires=i)

        # Entanglement
        for i in range(num_qubits-1):
            qml.CNOT(wires=[i, i+1])

    # Measuring in both X and Z bases
    return [qml.expval(qml.PauliX(i)) for i in range(num_qubits)] + [qml.expval(qml.PauliZ(i)) for i in range(num_qubits)]

"""## Hybrid Quantum-Classical Model

The reason for selecting a hybrid model is that after numerous attempts to build a purely quantum model, I realized that the quantum advantage for this specific problem is quite small, making it impractical for quantum computing alone to outperform classical methods.

However, through these experiments, I discovered that while quantum machine learning (QML) by itself can't surpass classical models, its feature extraction capabilities are extremely powerful. Quantum properties like entanglement and superposition help capture complex relationships between features, which can enhance the performance of classical models and provide deeper insights into the data.

For demonstration purposes, Iâ€™ve used the Random Forest algorithm alongside its quantum hybrid version.
"""

def generate_quantum_features(model, X):
    quantum_features = []
    for x in X:
        scaled_inputs = (x - np.min(x)) / (np.max(x) - np.min(x)) * 2 * np.pi
        quantum_output = quantum_circuit(scaled_inputs, np.random.uniform(0, 2*np.pi, (2, num_qubits, 2)))
        quantum_features.append(quantum_output)
    return np.array(quantum_features)

def quantumfit(model, X, y):
    quantum_features = generate_quantum_features(model, X)
    enhanced_features = np.hstack([X, quantum_features])
    model.fit(enhanced_features, y)

def qpredict(model, X):
    quantum_features = generate_quantum_features(model, X)
    enhanced_features = np.hstack([X, quantum_features])
    return model.predict(enhanced_features)

print("\nStarted training Hybrid Random Forest...")
hybrid_rf = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5, min_samples_leaf=2, random_state=42, n_jobs=-1)
quantumfit(hybrid_rf, X_train_scaled, y_train)
hybrid_pred = qpredict(hybrid_rf, X_test_scaled)
hybrid_accuracy = accuracy_score(y_test, hybrid_pred)
hybrid_precision = precision_score(y_test, hybrid_pred)
hybrid_recall = recall_score(y_test, hybrid_pred)
hybrid_f1 = f1_score(y_test, hybrid_pred)
hybrid_conf_matrix = confusion_matrix(y_test, hybrid_pred)

print(f"\nHybrid Random Forest Metrics:")
print(f"Accuracy: {hybrid_accuracy:.4f}")
print(f"Precision: {hybrid_precision:.4f}")
print(f"Recall: {hybrid_recall:.4f}")
print(f"F1 Score: {hybrid_f1:.4f}")
print("\nConfusion Matrix:")
print(hybrid_conf_matrix)

"""## Classical Random Forest for comparision"""

print("\nStarted training Classical Random Forest...")
classical_rf = RandomForestClassifier(n_estimators=105, max_depth=11, min_samples_split=6, min_samples_leaf=4,  random_state=41, n_jobs=-1)
classical_rf.fit(X_train_scaled, y_train)
classical_pred = classical_rf.predict(X_test_scaled)
classical_accuracy = accuracy_score(y_test, classical_pred)
classical_precision = precision_score(y_test, classical_pred)
classical_recall = recall_score(y_test, classical_pred)
classical_f1 = f1_score(y_test, classical_pred)
classical_conf_matrix = confusion_matrix(y_test, classical_pred)

print(f"\nClassical Random Forest Metrics:")
print(f"Accuracy: {classical_accuracy:.4f}")
print(f"Precision: {classical_precision:.4f}")
print(f"Recall: {classical_recall:.4f}")
print(f"F1 Score: {classical_f1:.4f}")
print("\nConfusion Matrix:")
print(classical_conf_matrix)

"""## Comparision and Proof of Concept"""

print("\nImprovement Analysis:")
improvement = (hybrid_accuracy - classical_accuracy) * 100
print(f"Accuracy improvement: {improvement:.2f}%")
improvement = (hybrid_precision - classical_precision) * 100
print(f"\nPrecision improvement: {improvement:.2f}%")
improvement = (hybrid_recall - classical_recall) * 100
print(f"\nRecall improvement: {improvement:.2f}%")
improvement = (hybrid_f1 - classical_f1) * 100
print(f"\nF1 Score improvement: {improvement:.2f}%")

"""## Export the Quantum Random Forest Model

"""

import joblib

# Save the trained model to a file
def export_model(model, filename):
    joblib.dump(model, filename)
    print(f"Model saved to {filename}")

# Example usage: Save the hybrid random forest model
export_model(hybrid_rf, "hybrid_random_forest_model.pkl")